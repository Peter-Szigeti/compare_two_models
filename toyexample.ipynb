{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_input = 6\n",
    "m = 1000\n",
    "\n",
    "X = np.random.normal(10, 3, (m, n_input)) # Sample random integers from 0 to 10 from a Gaussian distribution into an array of shape (m, n_input)\n",
    "y = np.sum(X * np.array([2, 3, 4, 5, 6, 7]), axis=1).reshape((-1,1)) # y = 2 * x_0 + 3 * x_1 + 4 * x_2 + 5 * x_3 + 6 * x_4 + 7 * x_5\n",
    "\n",
    "# add noise to X and y\n",
    "X += np.random.normal(0, 0.1, X.shape)\n",
    "y += np.random.normal(0, 0.1, y.shape)\n",
    "\n",
    "# normalize y and X between 0 and 1\n",
    "y = (y - np.min(y)) / (np.max(y) - np.min(y))\n",
    "X = (X - np.min(X, axis=0)) / (np.max(X, axis=0) - np.min(X, axis=0))\n",
    "\n",
    "X = np.hstack((np.ones((m, 1)), X)) # Add a column of ones to the left of X to account for the bias term\n",
    "\n",
    "# divide X and y into training and test sets\n",
    "X_train, X_test, y_train, y_test = X[:800], X[800:], y[:800], y[800:]\n",
    "# Approximate mapping 1\n",
    "W1 = np.random.uniform(0, 1, (n_input+1, 1))\n",
    "M1 = lambda X:X @ W1\n",
    "\n",
    "# Approximate mapping 2\n",
    "g = lambda x: 1 / (1 + np.exp(-x))\n",
    "ReLU = lambda x: np.maximum(x, 0)\n",
    "W2 = np.random.uniform(0, 1, (n_input+1, 1))\n",
    "M2 = lambda X:g(X @ W2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = lambda y, y_h: np.mean((y - y_h)**2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 0: 0.1215145247812986\n",
      "Loss at step 100: 0.019783505522634335\n",
      "Loss at step 200: 0.017371850438419097\n",
      "Loss at step 300: 0.015292380518996333\n",
      "Loss at step 400: 0.01349542585904482\n",
      "Loss at step 500: 0.01193870121485733\n",
      "Loss at step 600: 0.010586441373344873\n",
      "Loss at step 700: 0.00940850967306536\n",
      "Loss at step 800: 0.00837955624825859\n",
      "Loss at step 900: 0.007478262230916761\n",
      "Loss at step 1000: 0.006686682247983233\n",
      "Loss at step 1100: 0.005989684105200645\n",
      "Loss at step 1200: 0.005374477745348166\n",
      "Loss at step 1300: 0.004830222753093448\n",
      "Loss at step 1400: 0.004347703116431584\n",
      "Loss at step 1500: 0.003919058566103528\n",
      "Loss at step 1600: 0.003537562957518701\n",
      "Loss at step 1700: 0.0031974414712048938\n",
      "Loss at step 1800: 0.0028937196949295574\n",
      "Loss at step 1900: 0.00262209882133702\n",
      "Loss at step 2000: 0.00237885221425534\n",
      "Loss at step 2100: 0.002160739460445196\n",
      "Loss at step 2200: 0.001964934742360755\n",
      "Loss at step 2300: 0.001788966958708473\n",
      "Loss at step 2400: 0.0016306695020535534\n",
      "Loss at step 2500: 0.0014881379944345708\n",
      "Loss at step 2600: 0.0013596945989844521\n",
      "Loss at step 2700: 0.0012438577817335955\n",
      "Loss at step 2800: 0.001139316604657552\n",
      "Loss at step 2900: 0.0010449087981622628\n",
      "Loss at step 3000: 0.0009596019963409\n",
      "Loss at step 3100: 0.0008824776277715038\n",
      "Loss at step 3200: 0.0008127170434072575\n",
      "Loss at step 3300: 0.000749589535295752\n",
      "Loss at step 3400: 0.000692441958694939\n",
      "Loss at step 3500: 0.0006406897182271393\n",
      "Loss at step 3600: 0.0005938089181041856\n",
      "Loss at step 3700: 0.0005513295088272381\n",
      "Loss at step 3800: 0.0005128292894446134\n",
      "Loss at step 3900: 0.000477928646506717\n",
      "Loss at step 4000: 0.00044628592914642905\n",
      "Loss at step 4100: 0.0004175933749257555\n",
      "Loss at step 4200: 0.0003915735137822178\n",
      "Loss at step 4300: 0.0003679759880309555\n",
      "Loss at step 4400: 0.0003465747352960985\n",
      "Loss at step 4500: 0.0003271654887534095\n",
      "Loss at step 4600: 0.00030956355540765065\n",
      "Loss at step 4700: 0.000293601838499098\n",
      "Loss at step 4800: 0.0002791290746961899\n",
      "Loss at step 4900: 0.00026600826061780574\n",
      "Loss at step 5000: 0.0002541152465485056\n",
      "Loss at step 5100: 0.00024333747805341518\n",
      "Loss at step 5200: 0.00023357286864105468\n",
      "Loss at step 5300: 0.00022472878872441225\n",
      "Loss at step 5400: 0.00021672115794450626\n",
      "Loss at step 5500: 0.0002094736294898833\n",
      "Loss at step 5600: 0.00020291685640603296\n",
      "Loss at step 5700: 0.00019698783107093424\n",
      "Loss at step 5800: 0.0001916292900424434\n",
      "Loss at step 5900: 0.00018678917738159078\n",
      "Loss at step 6000: 0.000182420160341284\n",
      "Loss at step 6100: 0.0001784791919979939\n",
      "Loss at step 6200: 0.00017492711600778618\n",
      "Loss at step 6300: 0.00017172830919897847\n",
      "Loss at step 6400: 0.0001688503581811402\n",
      "Loss at step 6500: 0.00016626376656253056\n",
      "Loss at step 6600: 0.00016394168973236014\n",
      "Loss at step 6700: 0.00016185969448655985\n",
      "Loss at step 6800: 0.0001599955410613187\n",
      "Loss at step 6900: 0.00015832898539202701\n",
      "Loss at step 7000: 0.00015684159964036062\n",
      "Loss at step 7100: 0.00015551660923247712\n",
      "Loss at step 7200: 0.00015433874482964426\n",
      "Loss at step 7300: 0.00015329410781162595\n",
      "Loss at step 7400: 0.0001523700479951184\n",
      "Loss at step 7500: 0.00015155505243635812\n",
      "Loss at step 7600: 0.0001508386442805259\n",
      "Loss at step 7700: 0.00015021129072215797\n",
      "Loss at step 7800: 0.0001496643192318653\n",
      "Loss at step 7900: 0.00014918984128635572\n",
      "Loss at step 8000: 0.00014878068291212084\n",
      "Loss at step 8100: 0.00014843032141907323\n",
      "Loss at step 8200: 0.0001481328277597199\n",
      "Loss at step 8300: 0.00014788281400282638\n",
      "Loss at step 8400: 0.00014767538545860454\n",
      "Loss at step 8500: 0.0001475060970357896\n",
      "Loss at step 8600: 0.000147370913450073\n",
      "Loss at step 8700: 0.00014726617293864006\n",
      "Loss at step 8800: 0.00014718855416743238\n",
      "Loss at step 8900: 0.00014713504604656837\n",
      "Loss at step 9000: 0.0001471029201954063\n",
      "Loss at step 9100: 0.00014708970582229695\n",
      "Loss at step 9200: 0.000147093166805432\n",
      "Loss at step 9300: 0.00014711128078051256\n",
      "Loss at step 9400: 0.00014714222005849083\n",
      "Loss at step 9500: 0.00014718433421252028\n",
      "Loss at step 9600: 0.00014723613418767022\n",
      "Loss at step 9700: 0.0001472962778000322\n",
      "Loss at step 9800: 0.00014736355650372867\n",
      "Loss at step 9900: 0.000147436883315118\n"
     ]
    }
   ],
   "source": [
    "alpha = .1\n",
    "for i in range(10000):\n",
    "    y_h = M2(X_train) # ReLU mapping\n",
    "    # calculate the gradient, note that the activation function is not differentiable at 0\n",
    "    grad_W2 = -2 * np.mean((y_train - y_h) * X_train, axis=0).reshape(-1,1)\n",
    "    W2 = W2 - alpha * grad_W2\n",
    "    if i % 100 == 0:\n",
    "        print(\"Loss at step {}: {}\".format(i, loss_fn(y_train, y_h)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 of M1: -167.31645164534015\n",
      "R^2 of M2: 0.9949004265973724\n"
     ]
    }
   ],
   "source": [
    "# calculate R^2 of the test set\n",
    "y_h = M1(X_test)\n",
    "R2 = 1 - np.sum((y_test - y_h)**2) / np.sum((y_test - np.mean(y_test))**2)\n",
    "print(\"R^2 of M1: {}\".format(R2))\n",
    "\n",
    "y_h = M2(X_test)\n",
    "R2 = 1 - np.sum((y_test - y_h)**2) / np.sum((y_test - np.mean(y))**2)\n",
    "print(\"R^2 of M2: {}\".format(R2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
